{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data pre_processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0          0x28cc61\n",
       "3          0x2db41f\n",
       "15         0x2466f6\n",
       "23         0x23f9e9\n",
       "31         0x1fb4e1\n",
       "             ...   \n",
       "1867495    0x2c4dc2\n",
       "1867496    0x31be7c\n",
       "1867500    0x1ca58e\n",
       "1867515    0x35c8ba\n",
       "1867518    0x1d941b\n",
       "Name: tweet_id, Length: 411972, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "id = pd.read_csv(\"D:\\Kaggle\\dm2020-hw2-nthu\\data_identification.csv\",sep=',', header=0)\n",
    "train_id = id[id['identification']=='train'].tweet_id\n",
    "test_id = id[id['identification']=='test'].tweet_id\n",
    "test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         tweet_id       emotion\n",
       "0        0x3140b1       sadness\n",
       "1        0x368b73       disgust\n",
       "2        0x296183  anticipation\n",
       "3        0x2bd6e1           joy\n",
       "4        0x2ee1dd  anticipation\n",
       "...           ...           ...\n",
       "1455558  0x38dba0           joy\n",
       "1455559  0x300ea2           joy\n",
       "1455560  0x360b99          fear\n",
       "1455561  0x22eecf           joy\n",
       "1455562  0x2fb282  anticipation\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x3140b1</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x368b73</td>\n      <td>disgust</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x296183</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x2bd6e1</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2ee1dd</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455558</th>\n      <td>0x38dba0</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1455559</th>\n      <td>0x300ea2</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1455560</th>\n      <td>0x360b99</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>1455561</th>\n      <td>0x22eecf</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1455562</th>\n      <td>0x2fb282</td>\n      <td>anticipation</td>\n    </tr>\n  </tbody>\n</table>\n<p>1455563 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "labeled_emo = pd.read_csv(\"D:\\Kaggle\\dm2020-hw2-nthu\\emotion.csv\",sep=',', header=0)\n",
    "labeled_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = \"D:/Kaggle/dm2020-hw2-nthu/tweets_DM.json\"\n",
    "with open(file,'r') as r:\n",
    "    tweets =[]\n",
    "    for line in r.readlines():\n",
    "        tweets.append(json.loads(line))\n",
    "# converting json dataset from dictionary to dataframe\n",
    "raw = pd.DataFrame.from_dict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [_score, _index, _source, _crawldate, _type]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_score</th>\n      <th>_index</th>\n      <th>_source</th>\n      <th>_crawldate</th>\n      <th>_type</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "raw[raw['_index'] != 'hashtag_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [_score, _index, _source, _crawldate, _type]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_score</th>\n      <th>_index</th>\n      <th>_source</th>\n      <th>_crawldate</th>\n      <th>_type</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "raw[raw['_type'] != 'tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'hashtags': ['freepress', 'TrumpLegacy', 'CNN'],\n",
       " 'tweet_id': '0x2d5350',\n",
       " 'text': '@brianklaas As we see, Trump is dangerous to #freepress around the world. What a <LH> <LH> #TrumpLegacy.  #CNN'}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dic1 = raw['_source'][1]\n",
    "dic1['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row['_sorce']= row['_sorce'] lambda x : x['tweet']\n",
    "hashtage = [ i['tweet']['hashtags'] for i in raw['_source']]\n",
    "tweet_id = [ i['tweet']['tweet_id'] for i in raw['_source']]\n",
    "text = [ i['tweet']['text'] for i in raw['_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['hashtage']=hashtage\n",
    "raw['tweet_id']=tweet_id\n",
    "raw['text']=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         _score                         hashtage  tweet_id  \\\n",
       "0           391                       [Snapchat]  0x376b20   \n",
       "1           433    [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2           232                     [bibleverse]  0x28b412   \n",
       "3           376                               []  0x1cd5b0   \n",
       "4           989                               []  0x2de201   \n",
       "...         ...                              ...       ...   \n",
       "1867530     827  [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531     368                               []  0x29d0cb   \n",
       "1867532     498                               []  0x2a6a4f   \n",
       "1867533     840                               []  0x24faed   \n",
       "1867534     360                    [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  \n",
       "2        Confident of your obedience, I write to you, k...  \n",
       "3                      Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4        \"Trust is not the same as faith. A friend is s...  \n",
       "...                                                    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  \n",
       "1867531  I swear all this hard work gone pay off one da...  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  \n",
       "\n",
       "[1867535 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_score</th>\n      <th>hashtage</th>\n      <th>tweet_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>391</td>\n      <td>[Snapchat]</td>\n      <td>0x376b20</td>\n      <td>People who post \"add me on #Snapchat\" must be ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>433</td>\n      <td>[freepress, TrumpLegacy, CNN]</td>\n      <td>0x2d5350</td>\n      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>232</td>\n      <td>[bibleverse]</td>\n      <td>0x28b412</td>\n      <td>Confident of your obedience, I write to you, k...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>376</td>\n      <td>[]</td>\n      <td>0x1cd5b0</td>\n      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>989</td>\n      <td>[]</td>\n      <td>0x2de201</td>\n      <td>\"Trust is not the same as faith. A friend is s...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1867530</th>\n      <td>827</td>\n      <td>[mixedfeeling, butimTHATperson]</td>\n      <td>0x316b80</td>\n      <td>When you buy the last 2 tickets remaining for ...</td>\n    </tr>\n    <tr>\n      <th>1867531</th>\n      <td>368</td>\n      <td>[]</td>\n      <td>0x29d0cb</td>\n      <td>I swear all this hard work gone pay off one da...</td>\n    </tr>\n    <tr>\n      <th>1867532</th>\n      <td>498</td>\n      <td>[]</td>\n      <td>0x2a6a4f</td>\n      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n    </tr>\n    <tr>\n      <th>1867533</th>\n      <td>840</td>\n      <td>[]</td>\n      <td>0x24faed</td>\n      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n    </tr>\n    <tr>\n      <th>1867534</th>\n      <td>360</td>\n      <td>[Sundayvibes]</td>\n      <td>0x34be8c</td>\n      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n    </tr>\n  </tbody>\n</table>\n<p>1867535 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "process=raw.drop(['_crawldate','_index','_type','_source'],axis=1)\n",
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   tweet_id       emotion  _score             hashtage  \\\n",
       "0  0x3140b1       sadness     105                   []   \n",
       "1  0x368b73       disgust     797                   []   \n",
       "2  0x296183  anticipation     586  [pouroutyourspirit]   \n",
       "3  0x2bd6e1           joy     780                   []   \n",
       "4  0x2ee1dd  anticipation     329                   []   \n",
       "\n",
       "                                                text  \n",
       "0                             Why Chester? <LH> <LH>  \n",
       "1  @JaredLeto you are the fish that Jonah.  Excep...  \n",
       "2  He is coming back again and gonna come again q...  \n",
       "3  Dei is really such a beautiful person inside &...  \n",
       "4  Expressive praise is also an expression of fai...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>emotion</th>\n      <th>_score</th>\n      <th>hashtage</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x3140b1</td>\n      <td>sadness</td>\n      <td>105</td>\n      <td>[]</td>\n      <td>Why Chester? &lt;LH&gt; &lt;LH&gt;</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x368b73</td>\n      <td>disgust</td>\n      <td>797</td>\n      <td>[]</td>\n      <td>@JaredLeto you are the fish that Jonah.  Excep...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x296183</td>\n      <td>anticipation</td>\n      <td>586</td>\n      <td>[pouroutyourspirit]</td>\n      <td>He is coming back again and gonna come again q...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x2bd6e1</td>\n      <td>joy</td>\n      <td>780</td>\n      <td>[]</td>\n      <td>Dei is really such a beautiful person inside &amp;...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2ee1dd</td>\n      <td>anticipation</td>\n      <td>329</td>\n      <td>[]</td>\n      <td>Expressive praise is also an expression of fai...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "train_df =pd.merge(labeled_emo,process,left_on='tweet_id', right_on='tweet_id')\n",
    "train_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   tweet_id  _score          hashtage  \\\n",
       "0  0x28cc61     107                []   \n",
       "1  0x2db41f     728                []   \n",
       "2  0x2466f6     491    [womendrivers]   \n",
       "3  0x23f9e9      28  [robbingmembers]   \n",
       "4  0x1fb4e1     925                []   \n",
       "\n",
       "                                                text  \n",
       "0  @Habbo I've seen two separate colours of the e...  \n",
       "1  @FoxNews @KellyannePolls No serious self respe...  \n",
       "2  Looking for a new car, and it says 1 lady owne...  \n",
       "3  @cineworld “only the brave” just out and fount...  \n",
       "4  Felt like total dog 💩 going into open gym and ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>_score</th>\n      <th>hashtage</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x28cc61</td>\n      <td>107</td>\n      <td>[]</td>\n      <td>@Habbo I've seen two separate colours of the e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x2db41f</td>\n      <td>728</td>\n      <td>[]</td>\n      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x2466f6</td>\n      <td>491</td>\n      <td>[womendrivers]</td>\n      <td>Looking for a new car, and it says 1 lady owne...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x23f9e9</td>\n      <td>28</td>\n      <td>[robbingmembers]</td>\n      <td>@cineworld “only the brave” just out and fount...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x1fb4e1</td>\n      <td>925</td>\n      <td>[]</td>\n      <td>Felt like total dog 💩 going into open gym and ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "test_set =pd.DataFrame(test_id)\n",
    "test_df = pd.merge(test_set,process,left_on='tweet_id', right_on='tweet_id')\n",
    "test_df[0:5]"
   ]
  },
  {
   "source": [
    "# BOW Vectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1455563, 500)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_BOW_features_500\n",
    "y_train= train_df['emotion']\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = ['joy' for i in range(len(test_df['text']))]"
   ]
  },
  {
   "source": [
    "# Using LogisticRegression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "## training!\n",
    "clf_model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "## predict!\n",
    "y_train_pred = clf_model.predict(X_train)\n",
    "y_test_pred = clf_model.predict(X_test)\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR_training accuracy: 0.46\nLR_testing accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "print('LR_training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('LR_testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kaggle_demo1.csv','w') as w:\n",
    "    w.write('id,emotion\\n')\n",
    "    for i,j in zip(test_df['tweet_id'],y_test_pred):\n",
    "        w.write(i+','+j+'\\n')\n"
   ]
  },
  {
   "source": [
    "# DNN Method\n",
    "## Using Pre-training Glove Word2Vec\n",
    "## Using Text classification with Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                [Why, Chester, ?, <, LH, >, <, LH, >]\n",
       "1    [@, JaredLeto, you, are, the, fish, that, Jona...\n",
       "2    [He, is, coming, back, again, and, gon, na, co...\n",
       "Name: token, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "\n",
    "train_df['token'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "test_df['token'] = test_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df['token'][0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load pretrained word vectors\n",
    "# get a dict of tokens (key) and their pretrained word vectors (value)\n",
    "# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
    "dim = 0\n",
    "word_vecs= {}\n",
    "with open('D:/Word2Vec/glove/glove.twitter.27B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split()\n",
    "\n",
    "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
    "        if len(tokens) == 2:\n",
    "            dim = int(tokens[1])\n",
    "            continue\n",
    "    \n",
    "        word = tokens[0] \n",
    "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
    "        word_vecs[word] = vec\n",
    "# look up word vectors\n",
    "# turn each word into its pretrained word vector\n",
    "# return a list of word vectors corresponding to each token in train.data\n",
    "def Word2Vector(data_list, embedding_dict):\n",
    "    embedding_list = list()\n",
    "\n",
    "    # No Match Word (unknown word) Vector in Embedding\n",
    "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
    "\n",
    "    for idx_list in range(len(data_list)):\n",
    "        embedding_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            key = data_list[idx_list][idx_tuple][0] # token\n",
    "\n",
    "            if key in embedding_dict:\n",
    "                value = embedding_dict[key]\n",
    "            else:\n",
    "                value = unk_vector\n",
    "            embedding_list_tmp.append(value)\n",
    "        embedding_list.append(embedding_list_tmp)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vector(traindata_list, word_vecs)...\n",
      "Word2Vector(testdata_list, word_vecs)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Word2Vector(traindata_list, word_vecs)...')\n",
    "trainembed_list = Word2Vector(train_df['token'], word_vecs)\n",
    "print('Word2Vector(testdata_list, word_vecs)...')\n",
    "testembed_list = Word2Vector(test_df['token'], word_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "len(trainembed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([0.53220808, 0.06090363, 0.45627118, 0.4205117 , 0.13833629,\n",
       "        0.86486041, 0.92912644, 0.96014248, 0.11320715, 0.660196  ,\n",
       "        0.33268148, 0.50669442, 0.92907561, 0.4261753 , 0.9083427 ,\n",
       "        0.90329995, 0.90241052, 0.62823615, 0.8072786 , 0.17145464,\n",
       "        0.75900622, 0.42987173, 0.08832042, 0.45669124, 0.41205393,\n",
       "        0.50473491, 0.2236267 , 0.71758394, 0.0425141 , 0.08713762,\n",
       "        0.61706747, 0.01649812, 0.75161735, 0.02453985, 0.01602739,\n",
       "        0.74127032, 0.0149108 , 0.49584517, 0.14689757, 0.51235504,\n",
       "        0.39783266, 0.36120258, 0.97461113, 0.48731412, 0.53210786,\n",
       "        0.8289734 , 0.10084957, 0.03197739, 0.69071014, 0.12389173,\n",
       "        0.23238007, 0.84079452, 0.6089615 , 0.62258558, 0.22381935,\n",
       "        0.70389084, 0.22032252, 0.44575156, 0.57539032, 0.06705461,\n",
       "        0.18199977, 0.17761068, 0.66142716, 0.75462676, 0.08169912,\n",
       "        0.45172489, 0.42761314, 0.22384598, 0.41303066, 0.86328424,\n",
       "        0.68612812, 0.13194276, 0.62372889, 0.44515912, 0.80261433,\n",
       "        0.57916781, 0.09030546, 0.29764426, 0.21758302, 0.58266977,\n",
       "        0.78428625, 0.91837538, 0.3247466 , 0.62940532, 0.49143902,\n",
       "        0.70028962, 0.70779571, 0.64555667, 0.57159802, 0.2492584 ,\n",
       "        0.04907541, 0.30855942, 0.44176545, 0.08611237, 0.02813716,\n",
       "        0.84595079, 0.27371414, 0.16061168, 0.98394029, 0.75424847]),\n",
       " array([0.53220808, 0.06090363, 0.45627118, 0.4205117 , 0.13833629,\n",
       "        0.86486041, 0.92912644, 0.96014248, 0.11320715, 0.660196  ,\n",
       "        0.33268148, 0.50669442, 0.92907561, 0.4261753 , 0.9083427 ,\n",
       "        0.90329995, 0.90241052, 0.62823615, 0.8072786 , 0.17145464,\n",
       "        0.75900622, 0.42987173, 0.08832042, 0.45669124, 0.41205393,\n",
       "        0.50473491, 0.2236267 , 0.71758394, 0.0425141 , 0.08713762,\n",
       "        0.61706747, 0.01649812, 0.75161735, 0.02453985, 0.01602739,\n",
       "        0.74127032, 0.0149108 , 0.49584517, 0.14689757, 0.51235504,\n",
       "        0.39783266, 0.36120258, 0.97461113, 0.48731412, 0.53210786,\n",
       "        0.8289734 , 0.10084957, 0.03197739, 0.69071014, 0.12389173,\n",
       "        0.23238007, 0.84079452, 0.6089615 , 0.62258558, 0.22381935,\n",
       "        0.70389084, 0.22032252, 0.44575156, 0.57539032, 0.06705461,\n",
       "        0.18199977, 0.17761068, 0.66142716, 0.75462676, 0.08169912,\n",
       "        0.45172489, 0.42761314, 0.22384598, 0.41303066, 0.86328424,\n",
       "        0.68612812, 0.13194276, 0.62372889, 0.44515912, 0.80261433,\n",
       "        0.57916781, 0.09030546, 0.29764426, 0.21758302, 0.58266977,\n",
       "        0.78428625, 0.91837538, 0.3247466 , 0.62940532, 0.49143902,\n",
       "        0.70028962, 0.70779571, 0.64555667, 0.57159802, 0.2492584 ,\n",
       "        0.04907541, 0.30855942, 0.44176545, 0.08611237, 0.02813716,\n",
       "        0.84595079, 0.27371414, 0.16061168, 0.98394029, 0.75424847]),\n",
       " array([ 0.18764  , -0.23672  , -0.093466 ,  0.18942  ,  0.1447   ,\n",
       "         0.60422  , -0.10721  ,  0.59073  ,  0.4257   ,  0.71432  ,\n",
       "         0.12186  , -0.84222  , -3.355    ,  0.19039  ,  0.035512 ,\n",
       "         0.13611  ,  0.34534  , -0.58382  , -0.40108  ,  0.03415  ,\n",
       "         0.35675  ,  0.042792 , -0.12817  ,  0.63331  , -0.0099989,\n",
       "        -2.5911   , -0.27016  , -0.38807  ,  0.53367  , -0.14324  ,\n",
       "         0.19247  ,  1.2303   ,  0.17429  ,  0.30818  ,  0.19107  ,\n",
       "        -0.50054  ,  0.79188  ,  0.080798 , -0.30704  ,  0.12613  ,\n",
       "        -1.5086   ,  0.14417  , -0.29152  ,  0.52337  ,  0.092573 ,\n",
       "         0.11887  , -0.72877  ,  0.46835  , -0.073298 , -0.67389  ,\n",
       "         0.55402  , -0.092545 ,  0.015123 ,  0.12066  ,  0.027198 ,\n",
       "         0.46634  ,  0.077549 ,  0.4335   , -0.29856  ,  0.14527  ,\n",
       "        -0.29231  ,  0.30907  ,  0.43496  ,  0.023063 , -0.068818 ,\n",
       "        -0.3677   , -0.25194  , -0.52521  ,  0.18128  ,  0.14718  ,\n",
       "         0.66268  ,  1.1287   ,  0.17358  , -0.20029  , -0.4014   ,\n",
       "        -0.75654  , -0.54806  ,  0.65335  , -0.29535  ,  0.070593 ,\n",
       "         0.44817  , -0.31514  ,  0.38929  , -0.38943  ,  0.16661  ,\n",
       "        -0.41322  , -1.1952   ,  0.25349  , -0.59079  ,  0.47218  ,\n",
       "        -0.89528  , -0.14385  , -0.14595  ,  0.28922  , -0.42489  ,\n",
       "         0.010669 ,  0.15234  , -0.20056  ,  0.27297  , -0.15433  ]),\n",
       " array([-0.27836 , -0.43458 ,  0.44064 ,  0.33168 , -0.15686 ,  0.33991 ,\n",
       "         0.56526 ,  0.58363 , -0.71337 ,  0.70543 , -0.12519 , -0.14969 ,\n",
       "        -2.2262  ,  0.055424,  0.25214 , -1.1091  , -0.71484 , -0.4064  ,\n",
       "        -0.9338  ,  0.4656  , -0.18527 , -0.1525  ,  0.88851 ,  0.27675 ,\n",
       "         0.95344 , -1.5284  , -0.33765 , -0.69393 ,  1.3108  ,  0.30619 ,\n",
       "        -0.83176 , -0.61836 , -0.67975 ,  0.088536,  1.1637  , -0.54159 ,\n",
       "        -0.24793 , -0.36396 ,  0.11334 ,  0.063596, -1.7865  ,  0.22911 ,\n",
       "         0.76448 , -0.68906 , -0.20581 ,  0.24929 ,  0.023817, -0.28856 ,\n",
       "        -0.023921, -0.42972 , -0.60612 , -0.44249 ,  0.69629 ,  0.26608 ,\n",
       "         0.54362 ,  0.32994 , -0.43304 , -0.11857 ,  0.10659 , -0.37562 ,\n",
       "        -0.57538 , -0.79252 ,  0.63229 ,  0.085661,  0.061079, -0.17928 ,\n",
       "        -0.23929 ,  0.52237 , -0.010493,  0.43317 ,  0.3691  , -0.86263 ,\n",
       "         0.058366, -0.19403 , -0.37629 , -0.071214, -0.69021 ,  0.56134 ,\n",
       "        -0.082218, -0.67725 ,  1.3655  ,  0.18355 ,  0.43748 , -0.21538 ,\n",
       "         0.5281  ,  0.57029 , -1.5141  ,  0.51425 ,  0.10949 ,  0.45515 ,\n",
       "         0.12952 , -0.35161 , -0.58731 , -0.080694,  0.66473 , -0.71022 ,\n",
       "        -0.42284 , -0.22944 ,  0.73881 , -0.068071]),\n",
       " array([0.53220808, 0.06090363, 0.45627118, 0.4205117 , 0.13833629,\n",
       "        0.86486041, 0.92912644, 0.96014248, 0.11320715, 0.660196  ,\n",
       "        0.33268148, 0.50669442, 0.92907561, 0.4261753 , 0.9083427 ,\n",
       "        0.90329995, 0.90241052, 0.62823615, 0.8072786 , 0.17145464,\n",
       "        0.75900622, 0.42987173, 0.08832042, 0.45669124, 0.41205393,\n",
       "        0.50473491, 0.2236267 , 0.71758394, 0.0425141 , 0.08713762,\n",
       "        0.61706747, 0.01649812, 0.75161735, 0.02453985, 0.01602739,\n",
       "        0.74127032, 0.0149108 , 0.49584517, 0.14689757, 0.51235504,\n",
       "        0.39783266, 0.36120258, 0.97461113, 0.48731412, 0.53210786,\n",
       "        0.8289734 , 0.10084957, 0.03197739, 0.69071014, 0.12389173,\n",
       "        0.23238007, 0.84079452, 0.6089615 , 0.62258558, 0.22381935,\n",
       "        0.70389084, 0.22032252, 0.44575156, 0.57539032, 0.06705461,\n",
       "        0.18199977, 0.17761068, 0.66142716, 0.75462676, 0.08169912,\n",
       "        0.45172489, 0.42761314, 0.22384598, 0.41303066, 0.86328424,\n",
       "        0.68612812, 0.13194276, 0.62372889, 0.44515912, 0.80261433,\n",
       "        0.57916781, 0.09030546, 0.29764426, 0.21758302, 0.58266977,\n",
       "        0.78428625, 0.91837538, 0.3247466 , 0.62940532, 0.49143902,\n",
       "        0.70028962, 0.70779571, 0.64555667, 0.57159802, 0.2492584 ,\n",
       "        0.04907541, 0.30855942, 0.44176545, 0.08611237, 0.02813716,\n",
       "        0.84595079, 0.27371414, 0.16061168, 0.98394029, 0.75424847]),\n",
       " array([-2.7926e-01, -3.9774e-01,  3.4993e-02, -1.8978e-01,  2.2611e-02,\n",
       "         2.8441e-01,  6.8036e-01,  5.4030e-01, -7.4499e-01,  5.7992e-01,\n",
       "        -2.1540e-01, -1.8072e-01, -2.4405e+00, -2.5579e-01,  3.3012e-01,\n",
       "        -9.8366e-01, -7.8729e-01, -7.3706e-01, -6.7946e-01,  4.0387e-01,\n",
       "        -2.4585e-01, -2.4105e-01,  5.4311e-01,  1.2662e-02,  6.2936e-01,\n",
       "        -1.6794e+00, -4.1999e-01, -5.5115e-01,  1.2149e+00,  1.0322e-01,\n",
       "        -5.9495e-01, -3.3132e-01, -6.5915e-01, -2.6277e-02,  1.2254e+00,\n",
       "        -2.6589e-01, -1.2182e-01, -5.2818e-01, -9.5539e-02, -6.4713e-02,\n",
       "        -1.8080e+00,  1.1017e-01,  5.6426e-01, -5.5592e-01,  2.3939e-04,\n",
       "        -6.2990e-02,  1.1053e-02, -2.6551e-01, -2.2435e-01, -7.8056e-01,\n",
       "        -4.5214e-01, -5.2481e-01,  2.0492e-01,  6.6235e-01,  4.1222e-01,\n",
       "         5.4075e-01, -3.3322e-01, -2.2479e-01,  5.5999e-01, -3.6524e-01,\n",
       "        -5.4566e-01, -6.2881e-01,  3.2577e-01, -3.9052e-02, -6.0886e-03,\n",
       "        -8.7608e-02, -1.8300e-01,  5.1804e-01,  2.4097e-01,  6.8658e-01,\n",
       "         5.3169e-01, -7.6930e-01,  6.9634e-02, -1.5613e-01, -5.5900e-01,\n",
       "        -1.8223e-01, -8.0564e-01,  3.6769e-01,  7.6753e-02, -1.9963e-01,\n",
       "         1.3448e+00,  2.1072e-02,  1.7427e-01, -1.0656e-01,  2.8349e-01,\n",
       "         2.5018e-01, -1.4912e+00,  4.7959e-01,  1.5608e-01,  3.1644e-01,\n",
       "         1.3158e-01, -2.7540e-01, -2.2047e-01, -3.6555e-02,  7.5459e-01,\n",
       "        -6.0045e-01, -3.9063e-01, -2.0185e-01,  7.0267e-01, -7.4574e-02]),\n",
       " array([-0.27836 , -0.43458 ,  0.44064 ,  0.33168 , -0.15686 ,  0.33991 ,\n",
       "         0.56526 ,  0.58363 , -0.71337 ,  0.70543 , -0.12519 , -0.14969 ,\n",
       "        -2.2262  ,  0.055424,  0.25214 , -1.1091  , -0.71484 , -0.4064  ,\n",
       "        -0.9338  ,  0.4656  , -0.18527 , -0.1525  ,  0.88851 ,  0.27675 ,\n",
       "         0.95344 , -1.5284  , -0.33765 , -0.69393 ,  1.3108  ,  0.30619 ,\n",
       "        -0.83176 , -0.61836 , -0.67975 ,  0.088536,  1.1637  , -0.54159 ,\n",
       "        -0.24793 , -0.36396 ,  0.11334 ,  0.063596, -1.7865  ,  0.22911 ,\n",
       "         0.76448 , -0.68906 , -0.20581 ,  0.24929 ,  0.023817, -0.28856 ,\n",
       "        -0.023921, -0.42972 , -0.60612 , -0.44249 ,  0.69629 ,  0.26608 ,\n",
       "         0.54362 ,  0.32994 , -0.43304 , -0.11857 ,  0.10659 , -0.37562 ,\n",
       "        -0.57538 , -0.79252 ,  0.63229 ,  0.085661,  0.061079, -0.17928 ,\n",
       "        -0.23929 ,  0.52237 , -0.010493,  0.43317 ,  0.3691  , -0.86263 ,\n",
       "         0.058366, -0.19403 , -0.37629 , -0.071214, -0.69021 ,  0.56134 ,\n",
       "        -0.082218, -0.67725 ,  1.3655  ,  0.18355 ,  0.43748 , -0.21538 ,\n",
       "         0.5281  ,  0.57029 , -1.5141  ,  0.51425 ,  0.10949 ,  0.45515 ,\n",
       "         0.12952 , -0.35161 , -0.58731 , -0.080694,  0.66473 , -0.71022 ,\n",
       "        -0.42284 , -0.22944 ,  0.73881 , -0.068071]),\n",
       " array([0.53220808, 0.06090363, 0.45627118, 0.4205117 , 0.13833629,\n",
       "        0.86486041, 0.92912644, 0.96014248, 0.11320715, 0.660196  ,\n",
       "        0.33268148, 0.50669442, 0.92907561, 0.4261753 , 0.9083427 ,\n",
       "        0.90329995, 0.90241052, 0.62823615, 0.8072786 , 0.17145464,\n",
       "        0.75900622, 0.42987173, 0.08832042, 0.45669124, 0.41205393,\n",
       "        0.50473491, 0.2236267 , 0.71758394, 0.0425141 , 0.08713762,\n",
       "        0.61706747, 0.01649812, 0.75161735, 0.02453985, 0.01602739,\n",
       "        0.74127032, 0.0149108 , 0.49584517, 0.14689757, 0.51235504,\n",
       "        0.39783266, 0.36120258, 0.97461113, 0.48731412, 0.53210786,\n",
       "        0.8289734 , 0.10084957, 0.03197739, 0.69071014, 0.12389173,\n",
       "        0.23238007, 0.84079452, 0.6089615 , 0.62258558, 0.22381935,\n",
       "        0.70389084, 0.22032252, 0.44575156, 0.57539032, 0.06705461,\n",
       "        0.18199977, 0.17761068, 0.66142716, 0.75462676, 0.08169912,\n",
       "        0.45172489, 0.42761314, 0.22384598, 0.41303066, 0.86328424,\n",
       "        0.68612812, 0.13194276, 0.62372889, 0.44515912, 0.80261433,\n",
       "        0.57916781, 0.09030546, 0.29764426, 0.21758302, 0.58266977,\n",
       "        0.78428625, 0.91837538, 0.3247466 , 0.62940532, 0.49143902,\n",
       "        0.70028962, 0.70779571, 0.64555667, 0.57159802, 0.2492584 ,\n",
       "        0.04907541, 0.30855942, 0.44176545, 0.08611237, 0.02813716,\n",
       "        0.84595079, 0.27371414, 0.16061168, 0.98394029, 0.75424847]),\n",
       " array([-2.7926e-01, -3.9774e-01,  3.4993e-02, -1.8978e-01,  2.2611e-02,\n",
       "         2.8441e-01,  6.8036e-01,  5.4030e-01, -7.4499e-01,  5.7992e-01,\n",
       "        -2.1540e-01, -1.8072e-01, -2.4405e+00, -2.5579e-01,  3.3012e-01,\n",
       "        -9.8366e-01, -7.8729e-01, -7.3706e-01, -6.7946e-01,  4.0387e-01,\n",
       "        -2.4585e-01, -2.4105e-01,  5.4311e-01,  1.2662e-02,  6.2936e-01,\n",
       "        -1.6794e+00, -4.1999e-01, -5.5115e-01,  1.2149e+00,  1.0322e-01,\n",
       "        -5.9495e-01, -3.3132e-01, -6.5915e-01, -2.6277e-02,  1.2254e+00,\n",
       "        -2.6589e-01, -1.2182e-01, -5.2818e-01, -9.5539e-02, -6.4713e-02,\n",
       "        -1.8080e+00,  1.1017e-01,  5.6426e-01, -5.5592e-01,  2.3939e-04,\n",
       "        -6.2990e-02,  1.1053e-02, -2.6551e-01, -2.2435e-01, -7.8056e-01,\n",
       "        -4.5214e-01, -5.2481e-01,  2.0492e-01,  6.6235e-01,  4.1222e-01,\n",
       "         5.4075e-01, -3.3322e-01, -2.2479e-01,  5.5999e-01, -3.6524e-01,\n",
       "        -5.4566e-01, -6.2881e-01,  3.2577e-01, -3.9052e-02, -6.0886e-03,\n",
       "        -8.7608e-02, -1.8300e-01,  5.1804e-01,  2.4097e-01,  6.8658e-01,\n",
       "         5.3169e-01, -7.6930e-01,  6.9634e-02, -1.5613e-01, -5.5900e-01,\n",
       "        -1.8223e-01, -8.0564e-01,  3.6769e-01,  7.6753e-02, -1.9963e-01,\n",
       "         1.3448e+00,  2.1072e-02,  1.7427e-01, -1.0656e-01,  2.8349e-01,\n",
       "         2.5018e-01, -1.4912e+00,  4.7959e-01,  1.5608e-01,  3.1644e-01,\n",
       "         1.3158e-01, -2.7540e-01, -2.2047e-01, -3.6555e-02,  7.5459e-01,\n",
       "        -6.0045e-01, -3.9063e-01, -2.0185e-01,  7.0267e-01, -7.4574e-02])]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "trainembed_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(i) for i in trainembed_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 12\n",
    "embed_dim = 100  # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "vocab_size = 12000\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['emotion']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "y_train = label_encode(label_encoder, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(trainembed_list, maxlen=maxlen)"
   ]
  },
  {
   "source": [
    "### Somehow because of the hardware problem can`t finish traing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "csv_logger = CSVLogger('D:/Kaggle/dm2020-hw2-nthu/logs/training2_log.csv')\n",
    "# training!\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    )\n",
    "print('training finish')"
   ]
  },
  {
   "source": [
    "## TA DNN Method\n",
    "### using 500 BOW Feature as input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1455563, 500)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "\n",
    "X_train = train_data_BOW_features_500\n",
    "y_train= train_df['emotion']\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = ['joy' for i in range(len(test_df['text']))]\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'sadness': 193437,\n",
       "         'disgust': 139101,\n",
       "         'anticipation': 248935,\n",
       "         'joy': 516017,\n",
       "         'trust': 205478,\n",
       "         'anger': 39867,\n",
       "         'fear': 63999,\n",
       "         'surprise': 48729})"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from collections import Counter\n",
    "y_trains = Counter(y_train)\n",
    "y_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_shape:  500\noutput_shape:  8\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 500)]             0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 64)                32064     \n_________________________________________________________________\nre_lu (ReLU)                 (None, 64)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 64)                4160      \n_________________________________________________________________\nre_lu_1 (ReLU)               (None, 64)                0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 8)                 520       \n_________________________________________________________________\nsoftmax (Softmax)            (None, 8)                 0         \n=================================================================\nTotal params: 36,744\nTrainable params: 36,744\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "11372/11372 [==============================] - 35s 3ms/step - loss: 1.3357 - accuracy: 0.5131\n",
      "Epoch 2/10\n",
      "11372/11372 [==============================] - 35s 3ms/step - loss: 1.3341 - accuracy: 0.5137\n",
      "Epoch 3/10\n",
      "11372/11372 [==============================] - 36s 3ms/step - loss: 1.3330 - accuracy: 0.5142\n",
      "Epoch 4/10\n",
      "11372/11372 [==============================] - 37s 3ms/step - loss: 1.3323 - accuracy: 0.5144\n",
      "Epoch 5/10\n",
      "11372/11372 [==============================] - 36s 3ms/step - loss: 1.3317 - accuracy: 0.5142\n",
      "Epoch 6/10\n",
      "11372/11372 [==============================] - 36s 3ms/step - loss: 1.3312 - accuracy: 0.5146\n",
      "Epoch 7/10\n",
      "11372/11372 [==============================] - 37s 3ms/step - loss: 1.3309 - accuracy: 0.5145\n",
      "Epoch 8/10\n",
      "11372/11372 [==============================] - 37s 3ms/step - loss: 1.3306 - accuracy: 0.5148\n",
      "Epoch 9/10\n",
      "11372/11372 [==============================] - 37s 3ms/step - loss: 1.3302 - accuracy: 0.5150\n",
      "Epoch 10/10\n",
      "11372/11372 [==============================] - 35s 3ms/step - loss: 1.3298 - accuracy: 0.5149\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('D:/Kaggle/dm2020-hw2-nthu/logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    )\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.02936696, 0.04191859, 0.18879706, 0.10511594, 0.32717097,\n",
       "        0.16040786, 0.08082838, 0.06639429],\n",
       "       [0.02026086, 0.05082481, 0.17747216, 0.10967811, 0.2558774 ,\n",
       "        0.2685597 , 0.04311895, 0.07420798],\n",
       "       [0.03218394, 0.165647  , 0.3262833 , 0.01006515, 0.20612891,\n",
       "        0.09517279, 0.02554098, 0.13897792]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['joy', 'sadness', 'disgust', 'joy', 'joy'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kaggle_demo2.csv','w') as w:\n",
    "    w.write('id,emotion\\n')\n",
    "    for i,j in zip(test_df['tweet_id'],pred_result):\n",
    "        w.write(i+','+j+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}